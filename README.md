# üìä Proxmox Monitoring Stack - Ansible Playbook

This Ansible playbook automates the deployment of a monitoring stack using Prometheus and Grafana, specifically designed for monitoring:

üñ•Ô∏è - Proxmox VE hosts  
üöÄ - Virtual Machines (Linux for now)  
üì¶ - LXC Containers  
üìà - System metrics via Node Exporter

The stack provides comprehensive monitoring and visualization of:

üîç - Host system metrics  
üéØ - Container resources  
‚ö° - VM performance  
üè• - Proxmox cluster health  
üìä - Custom metrics via Node Exporter  
üö® - Automated alerts via AlertManager

This project can serve as a reference implementation for:

üè† - Homelab environments  
üåê - Small to medium Proxmox deployments  
üõ†Ô∏è - Self-hosted monitoring solutions  
üìö - Infrastructure automation examples

> **üöß Note**: This is an actively developed project. Future updates will include:
>
> ‚ú® - Additional dashboard templates  
> üîå - More exporters for specialized monitoring  
> üìñ - Extended documentation and best practices

## ‚ö° Prerequisites

üîß - Ansible installed on the control machine  
üíª - Target server running Ubuntu (adjust if using different OS)  
üîë - SSH access to the target server  
üçé - For macOS users: Set `OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES` environment variable

## üìÅ Directory Structure

```
monitoring-ansible/
‚îú‚îÄ‚îÄ inventory/
‚îÇ   ‚îî‚îÄ‚îÄ hosts.ini
‚îú‚îÄ‚îÄ group_vars/
‚îÇ   ‚îî‚îÄ‚îÄ all/
‚îÇ       ‚îú‚îÄ‚îÄ vars.yml
‚îÇ       ‚îî‚îÄ‚îÄ vault.yml
‚îú‚îÄ‚îÄ roles/
‚îÇ   ‚îú‚îÄ‚îÄ common/
‚îÇ   ‚îú‚îÄ‚îÄ monitoring_base/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îú‚îÄ‚îÄ node_exporter/
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/
‚îÇ       ‚îú‚îÄ‚îÄ prometheus/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml.j2
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml.j2
‚îÇ       ‚îî‚îÄ‚îÄ grafana/
‚îÇ           ‚îú‚îÄ‚îÄ docker-compose.yml.j2
‚îÇ           ‚îî‚îÄ‚îÄ grafana.ini.j2
‚îú‚îÄ‚îÄ site.yml
‚îî‚îÄ‚îÄ ansible.cfg
```

## üöÄ Getting Started

### 1Ô∏è‚É£ Clone and Configure Inventory

1. Clone this repository
2. Create your inventory file from the example:
   ```bash
   cp inventory/hosts.ini.example inventory/hosts.ini
   ```
3. Edit `inventory/hosts.ini` with your actual server details:
   - Update IP addresses
   - Set correct SSH users
   - Add/remove hosts as needed

### 2Ô∏è‚É£ Configure Vault

1. Create your vault file from the example:
   ```bash
   cp group_vars/all/vault.yml.example group_vars/all/vault.yml
   ```
2. Create a secure vault password:
   ```bash
   openssl rand -base64 64 > .vault_pass
   chmod 600 .vault_pass
   ```
3. Edit the vault file with your secrets:
   ```bash
   ansible-vault edit group_vars/all/vault.yml
   ```
   ‚ö†Ô∏è Make sure to change all default passwords!

### 3Ô∏è‚É£ Verify Configuration

1. Test your setup:

   ```bash
   # List all hosts to verify inventory
   ansible-inventory --list

   # Test connection to hosts
   ansible all -m ping
   ```

## üéÆ Usage

1. Run the playbook:

   ```bash
   ansible-playbook site.yml
   ```

2. For testing specific hosts:
   ```bash
   ansible-playbook site.yml --limit monitoring_hosts
   ```

## üåê Access Services

üîç - Prometheus: http://your-server-ip:9090
üìä - Grafana: http://your-server-ip:3000
üö® - AlertManager: http://your-server-ip:9093 (when enabled)

‚ö†Ô∏è Default Grafana credentials are in `group_vars/all/vault.yml` - make sure to change these in production!

## üîí Security Notes

1. Never commit sensitive files:
   - üìÅ `inventory/hosts.ini`
   - üîë `group_vars/all/vault.yml`
   - üîê `.vault_pass`
2. Always change default passwords before deploying to production
3. Use strong passwords for all services
4. Consider using different vault files for different environments

## üîß Adding New Secrets

1. Add new secrets to `group_vars/all/vault.yml`:

   ```bash
   ansible-vault edit group_vars/all/vault.yml
   ```

2. Reference them in `group_vars/all/vars.yml`:
   ```yaml
   my_variable: "{{ vault_my_variable }}"
   ```

## üö® Troubleshooting

If you encounter permission issues:

1. üîë Ensure your SSH key is properly set up
2. üëë Check that your user has sudo privileges
3. üîê Verify the vault password is correct

### macOS Users

If you're running this playbook from macOS and encounter fork safety issues with Node Exporter installation:

```bash
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
```

Add this to your `~/.zshrc` or `~/.bash_profile` to make it permanent:

```bash
echo 'export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES' >> ~/.zshrc
```

## üö® Alerting System

The monitoring stack includes an optional alerting system using AlertManager with Slack integration.
The system monitors by default:

### Host Availability

- üî¥ **HostDown**: Critical alert when a host is unreachable for more than 1 minute
- ‚ö†Ô∏è **HostHighDowntime**: Warning when host uptime falls below 95% in 24 hours

### Resource Usage

- üíª **HighCPUUsage**: Warning when CPU usage exceeds 85% for 5 minutes
- üß† **HighMemoryUsage**: Warning when memory usage exceeds 85% for 5 minutes
- üíæ **DiskSpaceRunningOut**: Warning when disk usage exceeds 85% for 5 minutes

### Alert Configuration

- Critical alerts repeat every hour
- Warning alerts repeat every 12 hours
- All alerts include host instance and service information
- Alerts are automatically resolved when conditions return to normal

### Setting Up Alerts

1. Configure Slack integration:

   ```bash
   # Edit vault file
   ansible-vault edit group_vars/all/vault.yml

   # Add Slack configuration
   vault_slack_webhook_url: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
   vault_slack_channel: "#alerts"
   ```

2. Alerts are automatically enabled when Slack webhook URL is configured
3. To disable alerts, remove or leave empty the `vault_slack_webhook_url`

## üîß Customizing Alerts

Alert thresholds can be adjusted in:

```
templates/monitoring/prometheus/rules/node_alerts.yml.j2
```

Common customizations:

- Adjust CPU/Memory thresholds (default: 85%)
- Change alert timing (default: 5m for resource alerts)
- Modify uptime requirements (default: 95% over 24h)
- Add new alert rules for specific metrics
